{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸŽ¯ LangGraph Aplicado a CodificaciÃ³n AutomÃ¡tica\n",
        "\n",
        "Este notebook muestra cÃ³mo **refactorizar tu sistema de codificaciÃ³n** usando LangGraph.\n",
        "\n",
        "## ðŸ“‹ Lo que vamos a hacer:\n",
        "\n",
        "1. **Analizar** tu flujo actual\n",
        "2. **DiseÃ±ar** el grafo de estados\n",
        "3. **Implementar** nodos clave\n",
        "4. **Probar** con datos reales\n",
        "5. **Comparar** con tu soluciÃ³n actual\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Setup completo\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Agregar ruta del proyecto\n",
        "project_root = Path.cwd().parent\n",
        "sys.path.append(str(project_root / \"backend\" / \"src\"))\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(project_root / \".env\")\n",
        "\n",
        "# Verificar\n",
        "assert os.getenv(\"OPENAI_API_KEY\"), \"âŒ Falta OPENAI_API_KEY\"\n",
        "print(\"âœ… Setup completo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Imports cargados\n"
          ]
        }
      ],
      "source": [
        "# Imports necesarios\n",
        "from typing import TypedDict, List, Dict, Literal\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "import pandas as pd\n",
        "\n",
        "print(\"âœ… Imports cargados\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“Š Tu Flujo Actual\n",
        "\n",
        "```\n",
        "Cargar Datos\n",
        "    â†“\n",
        "Por cada pregunta:\n",
        "    â†“\n",
        "    Dividir en batches (20 respuestas)\n",
        "    â†“\n",
        "    Por cada batch:\n",
        "        â†“\n",
        "        Llamar GPT con prompt hÃ­brido\n",
        "        â†“\n",
        "        Parsear respuesta\n",
        "        â†“\n",
        "        Acumular cÃ³digos nuevos\n",
        "    â†“\n",
        "    Normalizar cÃ³digos nuevos\n",
        "    â†“\n",
        "    Exportar a Excel\n",
        "```\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¨ DiseÃ±o del Grafo LangGraph\n",
        "\n",
        "```mermaid\n",
        "graph TD\n",
        "    START --> cargar[Cargar Datos]\n",
        "    cargar --> preparar[Preparar Batch]\n",
        "    preparar --> codificar[Codificar Batch]\n",
        "    codificar --> acumular[Acumular Resultados]\n",
        "    acumular --> decision{Â¿MÃ¡s batches?}\n",
        "    decision -->|SÃ­| preparar\n",
        "    decision -->|No| normalizar[Normalizar CÃ³digos]\n",
        "    normalizar --> exportar[Exportar Excel]\n",
        "    exportar --> END\n",
        "```\n",
        "\n",
        "**Ventajas:**\n",
        "- âœ… Cada nodo es testeable por separado\n",
        "- âœ… FÃ¡cil agregar pasos (ej: validaciÃ³n, retry)\n",
        "- âœ… Progreso rastreable nodo por nodo\n",
        "- âœ… Checkpointing automÃ¡tico\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ—ï¸ Definir el Estado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Estado definido\n",
            "\n",
            "ðŸ“Š Campos del estado: ['pregunta', 'modelo_gpt', 'batch_size', 'respuestas', 'catalogo', 'batch_actual', 'total_batches', 'batch_respuestas', 'codificaciones', 'codigos_nuevos_global', 'contador_codigo_nuevo', 'costo_total', 'tokens_total', 'mensaje_estado', 'progreso_pct']\n"
          ]
        }
      ],
      "source": [
        "class EstadoCodificacion(TypedDict):\n",
        "    \"\"\"Estado que fluye por el grafo\"\"\"\n",
        "    \n",
        "    # ConfiguraciÃ³n\n",
        "    pregunta: str\n",
        "    modelo_gpt: str\n",
        "    batch_size: int\n",
        "    \n",
        "    # Datos de entrada\n",
        "    respuestas: List[Dict]  # Lista de {texto, fila_excel}\n",
        "    catalogo: List[Dict]     # CatÃ¡logo histÃ³rico\n",
        "    \n",
        "    # Control de flujo\n",
        "    batch_actual: int\n",
        "    total_batches: int\n",
        "    batch_respuestas: List[Dict]  # Respuestas del batch actual\n",
        "    \n",
        "    # Resultados acumulados\n",
        "    codificaciones: List[Dict]  # Todas las codificaciones\n",
        "    codigos_nuevos_global: Dict[str, int]  # {descripcion: codigo_num}\n",
        "    contador_codigo_nuevo: int\n",
        "    \n",
        "    # MÃ©tricas\n",
        "    costo_total: float\n",
        "    tokens_total: int\n",
        "    \n",
        "    # Progreso\n",
        "    mensaje_estado: str\n",
        "    progreso_pct: float\n",
        "\n",
        "# Ejemplo de estado inicial\n",
        "estado_ejemplo = {\n",
        "    \"pregunta\": \"Â¿QuÃ© te gusta del producto?\",\n",
        "    \"modelo_gpt\": \"gpt-4o-mini\",\n",
        "    \"batch_size\": 20,\n",
        "    \"respuestas\": [],\n",
        "    \"catalogo\": [],\n",
        "    \"batch_actual\": 0,\n",
        "    \"total_batches\": 0,\n",
        "    \"batch_respuestas\": [],\n",
        "    \"codificaciones\": [],\n",
        "    \"codigos_nuevos_global\": {},\n",
        "    \"contador_codigo_nuevo\": 1,\n",
        "    \"costo_total\": 0.0,\n",
        "    \"tokens_total\": 0,\n",
        "    \"mensaje_estado\": \"Iniciando...\",\n",
        "    \"progreso_pct\": 0.0\n",
        "}\n",
        "\n",
        "print(\"âœ… Estado definido\")\n",
        "print(f\"\\nðŸ“Š Campos del estado: {list(estado_ejemplo.keys())}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ”¨ Implementar Nodos\n",
        "\n",
        "### Nodo 1: Cargar Datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ§ª Probando nodo_cargar_datos:\n",
            "ðŸ“‚ Cargados: 5 respuestas\n",
            "ðŸ“š CatÃ¡logo: 3 cÃ³digos\n",
            "ðŸ“¦ Total batches: 1\n",
            "Estado actualizado: total_batches=1\n"
          ]
        }
      ],
      "source": [
        "def nodo_cargar_datos(state: EstadoCodificacion) -> EstadoCodificacion:\n",
        "    \"\"\"\n",
        "    Simula carga de datos (en producciÃ³n leerÃ­as de Excel)\n",
        "    \"\"\"\n",
        "    # Datos de ejemplo\n",
        "    respuestas_ejemplo = [\n",
        "        {\"texto\": \"Me gusta el sabor\", \"fila_excel\": 1},\n",
        "        {\"texto\": \"Es muy versÃ¡til para cocinar\", \"fila_excel\": 2},\n",
        "        {\"texto\": \"Tiene buen precio y calidad\", \"fila_excel\": 3},\n",
        "        {\"texto\": \"El sabor es excelente\", \"fila_excel\": 4},\n",
        "        {\"texto\": \"Lo uso en muchas comidas diferentes\", \"fila_excel\": 5},\n",
        "    ]\n",
        "    \n",
        "    catalogo_ejemplo = [\n",
        "        {\"codigo\": 1, \"descripcion\": \"Sabor agradable\"},\n",
        "        {\"codigo\": 2, \"descripcion\": \"Buen precio\"},\n",
        "        {\"codigo\": 3, \"descripcion\": \"Calidad del producto\"},\n",
        "    ]\n",
        "    \n",
        "    total_batches = (len(respuestas_ejemplo) + state[\"batch_size\"] - 1) // state[\"batch_size\"]\n",
        "    \n",
        "    print(f\" Cargados: {len(respuestas_ejemplo)} respuestas\")\n",
        "    print(f\" CatÃ¡logo: {len(catalogo_ejemplo)} cÃ³digos\")\n",
        "    print(f\" Total batches: {total_batches}\")\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"respuestas\": respuestas_ejemplo,\n",
        "        \"catalogo\": catalogo_ejemplo,\n",
        "        \"total_batches\": total_batches,\n",
        "        \"mensaje_estado\": f\"Datos cargados: {len(respuestas_ejemplo)} respuestas\"\n",
        "    }\n",
        "\n",
        "# Probar nodo\n",
        "print(\"\\n Probando nodo_cargar_datos:\")\n",
        "resultado = nodo_cargar_datos(estado_ejemplo)\n",
        "print(f\"Estado actualizado: total_batches={resultado['total_batches']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nodo 2: Preparar Batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ðŸ§ª Probando nodo_preparar_batch:\n",
            "\n",
            "ðŸ“¦ Batch 1/1\n",
            "   Respuestas 1 a 5\n",
            "   Progreso: 100.0%\n",
            "Batch extraÃ­do: 5 respuestas\n"
          ]
        }
      ],
      "source": [
        "def nodo_preparar_batch(state: EstadoCodificacion) -> EstadoCodificacion:\n",
        "    \"\"\"\n",
        "    Extrae el siguiente batch de respuestas\n",
        "    \"\"\"\n",
        "    inicio = state[\"batch_actual\"] * state[\"batch_size\"]\n",
        "    fin = min(inicio + state[\"batch_size\"], len(state[\"respuestas\"]))\n",
        "    \n",
        "    batch = state[\"respuestas\"][inicio:fin]\n",
        "    \n",
        "    progreso = ((state[\"batch_actual\"] + 1) / state[\"total_batches\"]) * 100\n",
        "    \n",
        "    print(f\"\\nðŸ“¦ Batch {state['batch_actual'] + 1}/{state['total_batches']}\")\n",
        "    print(f\"   Respuestas {inicio+1} a {fin}\")\n",
        "    print(f\"   Progreso: {progreso:.1f}%\")\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"batch_respuestas\": batch,\n",
        "        \"progreso_pct\": progreso,\n",
        "        \"mensaje_estado\": f\"Procesando batch {state['batch_actual'] + 1}/{state['total_batches']}\"\n",
        "    }\n",
        "\n",
        "# Probar\n",
        "print(\"\\nðŸ§ª Probando nodo_preparar_batch:\")\n",
        "estado_test = {**resultado, \"batch_actual\": 0}\n",
        "resultado2 = nodo_preparar_batch(estado_test)\n",
        "print(f\"Batch extraÃ­do: {len(resultado2['batch_respuestas'])} respuestas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nodo 3: Codificar Batch (con GPT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Esquemas Pydantic definidos\n"
          ]
        }
      ],
      "source": [
        "# Definir esquemas Pydantic para respuesta estructurada\n",
        "class ResultadoCodificacionGPT(BaseModel):\n",
        "    respuesta_id: int = Field(description=\"ID de la respuesta (1-based)\")\n",
        "    decision: Literal[\"historico\", \"nuevo\", \"mixto\", \"rechazar\"] = Field(\n",
        "        description=\"Tipo de codificaciÃ³n\"\n",
        "    )\n",
        "    codigos_historicos: List[int] = Field(\n",
        "        default_factory=list,\n",
        "        description=\"CÃ³digos del catÃ¡logo histÃ³rico\"\n",
        "    )\n",
        "    codigos_nuevos: List[str] = Field(\n",
        "        default_factory=list,\n",
        "        description=\"Descripciones de cÃ³digos nuevos\"\n",
        "    )\n",
        "    justificacion: str = Field(description=\"Por quÃ© se asignaron estos cÃ³digos\")\n",
        "\n",
        "class RespuestaBatchGPT(BaseModel):\n",
        "    codificaciones: List[ResultadoCodificacionGPT] = Field(\n",
        "        description=\"Lista de codificaciones para el batch\"\n",
        "    )\n",
        "\n",
        "print(\"âœ… Esquemas Pydantic definidos\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nodo_codificar_batch(state: EstadoCodificacion) -> EstadoCodificacion:\n",
        "    \"\"\"\n",
        "    Llama a GPT para codificar el batch actual\n",
        "    \"\"\"\n",
        "    # Preparar prompt\n",
        "    catalogo_str = \"\\n\".join([\n",
        "        f\"- CÃ³digo {c['codigo']}: {c['descripcion']}\"\n",
        "        for c in state[\"catalogo\"]\n",
        "    ])\n",
        "    \n",
        "    respuestas_str = \"\\n\".join([\n",
        "        f\"{i+1}. {r['texto']}\"\n",
        "        for i, r in enumerate(state[\"batch_respuestas\"])\n",
        "    ])\n",
        "    \n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"Eres un experto en codificaciÃ³n de respuestas abiertas.\n",
        "\n",
        "CATÃLOGO HISTÃ“RICO:\n",
        "{catalogo}\n",
        "\n",
        "INSTRUCCIONES:\n",
        "1. Para cada respuesta, decide:\n",
        "   - \"historico\": Si hay cÃ³digos del catÃ¡logo que apliquen\n",
        "   - \"nuevo\": Si se necesitan cÃ³digos nuevos (ideas no en catÃ¡logo)\n",
        "   - \"mixto\": CombinaciÃ³n de histÃ³ricos y nuevos\n",
        "   - \"rechazar\": Si la respuesta no es vÃ¡lida\n",
        "\n",
        "2. Puedes asignar MÃšLTIPLES cÃ³digos si hay varias ideas.\n",
        "\n",
        "3. Para cÃ³digos nuevos, sÃ© especÃ­fico pero no redundante:\n",
        "   âœ… \"Versatilidad de uso\"\n",
        "   âŒ \"Versatilidad de uso en comidas\" (muy especÃ­fico)\n",
        "   âŒ \"Versatilidad\" (muy genÃ©rico)\n",
        "\n",
        "Responde en formato JSON estructurado.\"\"\"),\n",
        "        (\"user\", \"PREGUNTA: {pregunta}\\n\\nRESPUESTAS:\\n{respuestas}\")\n",
        "    ])\n",
        "    \n",
        "    # Llamar a GPT\n",
        "    llm = ChatOpenAI(model=state[\"modelo_gpt\"], temperature=0)\n",
        "    chain = prompt | llm.with_structured_output(RespuestaBatchGPT)\n",
        "    \n",
        "    print(f\"\\nðŸ¤– Llamando a {state['modelo_gpt']}...\")\n",
        "    \n",
        "    resultado = chain.invoke({\n",
        "        \"catalogo\": catalogo_str,\n",
        "        \"pregunta\": state[\"pregunta\"],\n",
        "        \"respuestas\": respuestas_str\n",
        "    })\n",
        "    \n",
        "    print(f\"âœ… GPT respondiÃ³ con {len(resultado.codificaciones)} codificaciones\")\n",
        "    \n",
        "    # Mostrar algunas\n",
        "    for cod in resultado.codificaciones[:2]:\n",
        "        print(f\"   - Respuesta {cod.respuesta_id}: {cod.decision}\")\n",
        "        if cod.codigos_historicos:\n",
        "            print(f\"     HistÃ³ricos: {cod.codigos_historicos}\")\n",
        "        if cod.codigos_nuevos:\n",
        "            print(f\"     Nuevos: {cod.codigos_nuevos}\")\n",
        "    \n",
        "    # Convertir a formato interno\n",
        "    codificaciones_batch = []\n",
        "    for cod in resultado.codificaciones:\n",
        "        codificaciones_batch.append({\n",
        "            \"fila_excel\": state[\"batch_respuestas\"][cod.respuesta_id - 1][\"fila_excel\"],\n",
        "            \"texto\": state[\"batch_respuestas\"][cod.respuesta_id - 1][\"texto\"],\n",
        "            \"decision\": cod.decision,\n",
        "            \"codigos_historicos\": cod.codigos_historicos,\n",
        "            \"codigos_nuevos\": cod.codigos_nuevos,\n",
        "            \"justificacion\": cod.justificacion\n",
        "        })\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"codificaciones\": state[\"codificaciones\"] + codificaciones_batch,\n",
        "        \"mensaje_estado\": f\"Batch {state['batch_actual'] + 1} codificado\"\n",
        "    }\n",
        "\n",
        "# NO probamos aÃºn (consume API)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nodo 4: Incrementar Batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nodo_incrementar_batch(state: EstadoCodificacion) -> EstadoCodificacion:\n",
        "    \"\"\"\n",
        "    Incrementa el contador de batch\n",
        "    \"\"\"\n",
        "    nuevo_batch = state[\"batch_actual\"] + 1\n",
        "    \n",
        "    print(f\"âž¡ï¸  Avanzando a batch {nuevo_batch + 1}\")\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"batch_actual\": nuevo_batch\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nodo 5: Normalizar CÃ³digos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def similitud_jaccard(desc1: str, desc2: str) -> float:\n",
        "    \"\"\"Calcula similitud entre descripciones\"\"\"\n",
        "    palabras1 = set(desc1.lower().split())\n",
        "    palabras2 = set(desc2.lower().split())\n",
        "    \n",
        "    interseccion = len(palabras1 & palabras2)\n",
        "    union = len(palabras1 | palabras2)\n",
        "    \n",
        "    return interseccion / union if union > 0 else 0.0\n",
        "\n",
        "def nodo_normalizar(state: EstadoCodificacion) -> EstadoCodificacion:\n",
        "    \"\"\"\n",
        "    Unifica cÃ³digos nuevos redundantes\n",
        "    \"\"\"\n",
        "    print(\"\\nðŸ”§ Normalizando cÃ³digos nuevos...\")\n",
        "    \n",
        "    # Recolectar todas las descripciones nuevas\n",
        "    descripciones_vistas = {}\n",
        "    mapeo = {}  # {descripcion_original: descripcion_normalizada}\n",
        "    contador = 1\n",
        "    \n",
        "    for cod in state[\"codificaciones\"]:\n",
        "        for desc in cod[\"codigos_nuevos\"]:\n",
        "            # Buscar si ya existe similar\n",
        "            encontrado = False\n",
        "            for desc_norm, _ in descripciones_vistas.items():\n",
        "                if similitud_jaccard(desc, desc_norm) > 0.6:\n",
        "                    mapeo[desc] = desc_norm\n",
        "                    encontrado = True\n",
        "                    print(f\"   â™»ï¸  '{desc}' â†’ '{desc_norm}' (similitud)\")\n",
        "                    break\n",
        "            \n",
        "            if not encontrado:\n",
        "                mapeo[desc] = desc\n",
        "                descripciones_vistas[desc] = contador\n",
        "                contador += 1\n",
        "    \n",
        "    # Aplicar normalizaciÃ³n\n",
        "    codigos_nuevos_global = {}\n",
        "    contador_nuevo = 1\n",
        "    \n",
        "    for desc_norm in descripciones_vistas.keys():\n",
        "        codigo_final = f\"N{contador_nuevo}\"\n",
        "        codigos_nuevos_global[desc_norm] = codigo_final\n",
        "        contador_nuevo += 1\n",
        "    \n",
        "    print(f\"âœ… {len(codigos_nuevos_global)} cÃ³digos nuevos Ãºnicos\")\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"codigos_nuevos_global\": codigos_nuevos_global,\n",
        "        \"contador_codigo_nuevo\": contador_nuevo,\n",
        "        \"mensaje_estado\": \"CÃ³digos normalizados\"\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Nodo 6: Exportar Resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nodo_exportar(state: EstadoCodificacion) -> EstadoCodificacion:\n",
        "    \"\"\"\n",
        "    Prepara DataFrame para exportar\n",
        "    \"\"\"\n",
        "    print(\"\\nðŸ’¾ Preparando exportaciÃ³n...\")\n",
        "    \n",
        "    filas = []\n",
        "    for cod in state[\"codificaciones\"]:\n",
        "        # Combinar cÃ³digos\n",
        "        codigos_finales = []\n",
        "        \n",
        "        # HistÃ³ricos\n",
        "        if cod[\"codigos_historicos\"]:\n",
        "            codigos_finales.extend([str(c) for c in cod[\"codigos_historicos\"]])\n",
        "        \n",
        "        # Nuevos\n",
        "        if cod[\"codigos_nuevos\"]:\n",
        "            for desc in cod[\"codigos_nuevos\"]:\n",
        "                desc_norm = desc  # Ya normalizado en paso anterior\n",
        "                codigo_nuevo = state[\"codigos_nuevos_global\"].get(desc_norm, \"?\")\n",
        "                codigos_finales.append(codigo_nuevo)\n",
        "        \n",
        "        filas.append({\n",
        "            \"fila\": cod[\"fila_excel\"],\n",
        "            \"respuesta\": cod[\"texto\"],\n",
        "            \"decision\": cod[\"decision\"],\n",
        "            \"codigos\": \" | \".join(codigos_finales) if codigos_finales else \"SIN_CODIGO\",\n",
        "            \"justificacion\": cod[\"justificacion\"]\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(filas)\n",
        "    \n",
        "    print(f\"âœ… DataFrame creado: {len(df)} filas\")\n",
        "    print(f\"\\nðŸ“Š Primeras filas:\")\n",
        "    print(df.head())\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"mensaje_estado\": \"ExportaciÃ³n completa\",\n",
        "        \"progreso_pct\": 100.0\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Grafo de codificaciÃ³n compilado\n"
          ]
        }
      ],
      "source": [
        "# FunciÃ³n de decisiÃ³n: Â¿continuar o terminar?\n",
        "def decidir_continuar(state: EstadoCodificacion) -> str:\n",
        "    if state[\"batch_actual\"] < state[\"total_batches\"]:\n",
        "        return \"preparar_batch\"\n",
        "    else:\n",
        "        return \"normalizar\"\n",
        "\n",
        "# Construir grafo\n",
        "workflow = StateGraph(EstadoCodificacion)\n",
        "\n",
        "# Agregar nodos\n",
        "workflow.add_node(\"cargar\", nodo_cargar_datos)\n",
        "workflow.add_node(\"preparar_batch\", nodo_preparar_batch)\n",
        "workflow.add_node(\"codificar\", nodo_codificar_batch)\n",
        "workflow.add_node(\"incrementar\", nodo_incrementar_batch)\n",
        "workflow.add_node(\"normalizar\", nodo_normalizar)\n",
        "workflow.add_node(\"exportar\", nodo_exportar)\n",
        "\n",
        "# Definir flujo\n",
        "workflow.set_entry_point(\"cargar\")\n",
        "workflow.add_edge(\"cargar\", \"preparar_batch\")\n",
        "workflow.add_edge(\"preparar_batch\", \"codificar\")\n",
        "workflow.add_edge(\"codificar\", \"incrementar\")\n",
        "\n",
        "# DecisiÃ³n: Â¿mÃ¡s batches?\n",
        "workflow.add_conditional_edges(\n",
        "    \"incrementar\",\n",
        "    decidir_continuar,\n",
        "    {\n",
        "        \"preparar_batch\": \"preparar_batch\",\n",
        "        \"normalizar\": \"normalizar\"\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"normalizar\", \"exportar\")\n",
        "workflow.add_edge(\"exportar\", END)\n",
        "\n",
        "# Compilar\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"âœ… Grafo de codificaciÃ³n compilado\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ§ª Ejecutar el Grafo (ADVERTENCIA: Consume API)\n",
        "\n",
        "âš ï¸ **Esta celda llamarÃ¡ a OpenAI GPT** - descomenta para ejecutar:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Ejecutando grafo completo...\n",
            "\n",
            "============================================================\n",
            "ðŸ“‚ Cargados: 5 respuestas\n",
            "ðŸ“š CatÃ¡logo: 3 cÃ³digos\n",
            "ðŸ“¦ Total batches: 2\n",
            "\n",
            "ðŸ“¦ Batch 1/2\n",
            "   Respuestas 1 a 3\n",
            "   Progreso: 50.0%\n",
            "\n",
            "ðŸ¤– Llamando a gpt-4o-mini...\n",
            "âœ… GPT respondiÃ³ con 3 codificaciones\n",
            "   - Respuesta 1: historico\n",
            "     HistÃ³ricos: [1]\n",
            "   - Respuesta 2: nuevo\n",
            "     Nuevos: ['Versatilidad de uso']\n",
            "âž¡ï¸  Avanzando a batch 2\n",
            "\n",
            "ðŸ“¦ Batch 2/2\n",
            "   Respuestas 4 a 5\n",
            "   Progreso: 100.0%\n",
            "\n",
            "ðŸ¤– Llamando a gpt-4o-mini...\n",
            "âœ… GPT respondiÃ³ con 2 codificaciones\n",
            "   - Respuesta 1: historico\n",
            "     HistÃ³ricos: [1]\n",
            "   - Respuesta 2: nuevo\n",
            "     Nuevos: ['Versatilidad de uso']\n",
            "âž¡ï¸  Avanzando a batch 3\n",
            "\n",
            "ðŸ”§ Normalizando cÃ³digos nuevos...\n",
            "   â™»ï¸  'Versatilidad de uso' â†’ 'Versatilidad de uso' (similitud)\n",
            "âœ… 1 cÃ³digos nuevos Ãºnicos\n",
            "\n",
            "ðŸ’¾ Preparando exportaciÃ³n...\n",
            "âœ… DataFrame creado: 5 filas\n",
            "\n",
            "ðŸ“Š Primeras filas:\n",
            "   fila                            respuesta   decision codigos  \\\n",
            "0     1                    Me gusta el sabor  historico       1   \n",
            "1     2         Es muy versÃ¡til para cocinar      nuevo      N1   \n",
            "2     3          Tiene buen precio y calidad      mixto   2 | 3   \n",
            "3     4                El sabor es excelente  historico       1   \n",
            "4     5  Lo uso en muchas comidas diferentes      nuevo      N1   \n",
            "\n",
            "                                       justificacion  \n",
            "0  El comentario menciona el sabor, que se relaci...  \n",
            "1  La respuesta destaca la versatilidad del produ...  \n",
            "2  La respuesta menciona tanto el buen precio com...  \n",
            "3  El sabor excelente se relaciona directamente c...  \n",
            "4  La respuesta menciona el uso en muchas comidas...  \n",
            "\\n============================================================\n",
            "ðŸŽ‰ Â¡COMPLETADO!\n",
            "============================================================\n",
            "âœ… 5 respuestas codificadas\n",
            "âœ… 1 cÃ³digos nuevos\n",
            "âš ï¸ Celda comentada - descomenta para ejecutar\n"
          ]
        }
      ],
      "source": [
        "# Descomenta para ejecutar:\n",
        "estado_inicial = {\n",
        "    \"pregunta\": \"Â¿QuÃ© te gusta del producto?\",\n",
        "    \"modelo_gpt\": \"gpt-4o-mini\",\n",
        "    \"batch_size\": 3,  # PequeÃ±o para prueba\n",
        "    \"respuestas\": [],\n",
        "    \"catalogo\": [],\n",
        "    \"batch_actual\": 0,\n",
        "    \"total_batches\": 0,\n",
        "    \"batch_respuestas\": [],\n",
        "    \"codificaciones\": [],\n",
        "    \"codigos_nuevos_global\": {},\n",
        "    \"contador_codigo_nuevo\": 1,\n",
        "    \"costo_total\": 0.0,\n",
        "    \"tokens_total\": 0,\n",
        "    \"mensaje_estado\": \"Iniciando...\",\n",
        "    \"progreso_pct\": 0.0\n",
        "}\n",
        "\n",
        "print(\"ðŸš€ Ejecutando grafo completo...\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "resultado_final = app.invoke(estado_inicial)\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"ðŸŽ‰ Â¡COMPLETADO!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"âœ… {len(resultado_final['codificaciones'])} respuestas codificadas\")\n",
        "print(f\"âœ… {len(resultado_final['codigos_nuevos_global'])} cÃ³digos nuevos\")\n",
        "\n",
        "\n",
        "print(\"âš ï¸ Celda comentada - descomenta para ejecutar\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸŽ¯ Ventajas de LangGraph vs. Tu CÃ³digo Actual\n",
        "\n",
        "| Aspecto | CÃ³digo Actual | Con LangGraph |\n",
        "|---------|--------------|---------------|\n",
        "| **OrganizaciÃ³n** | Todo en `codificador.py` | Cada nodo = funciÃ³n independiente |\n",
        "| **Testing** | DifÃ­cil testear pasos intermedios | Cada nodo se prueba por separado |\n",
        "| **Progreso** | Callback manual | Nativo en cada transiciÃ³n |\n",
        "| **Reintentos** | Implementar manualmente | Retry automÃ¡tico por nodo |\n",
        "| **Checkpointing** | No hay | Guardar/recuperar estado |\n",
        "| **Observabilidad** | Logs bÃ¡sicos | Trazas completas (LangSmith) |\n",
        "| **ModificaciÃ³n** | Tocar lÃ³gica existente | Agregar/quitar nodos |\n",
        "| **Debugging** | Print statements | Ver estado en cada paso |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸš€ PrÃ³ximos Pasos\n",
        "\n",
        "### 1. **Mejorar este notebook:**\n",
        "   - Agregar nodo de validaciÃ³n (detectar respuestas invÃ¡lidas)\n",
        "   - Implementar retry logic si GPT falla\n",
        "   - Agregar cÃ¡lculo de mÃ©tricas (costos, tokens)\n",
        "\n",
        "### 2. **Migrar a producciÃ³n:**\n",
        "   - Mover nodos a `backend/src/cod_backend/core/langgraph_nodes.py`\n",
        "   - Crear grafo en `backend/src/cod_backend/core/langgraph_workflow.py`\n",
        "   - Actualizar API para usar el grafo\n",
        "\n",
        "### 3. **Agregar funcionalidades avanzadas:**\n",
        "   - **Streaming**: Enviar progreso en tiempo real al frontend (SSE)\n",
        "   - **Checkpointing**: Guardar progreso y recuperar si falla\n",
        "   - **Observabilidad**: Integrar LangSmith para trazas\n",
        "   - **Multi-agente**: Un agente clasifica, otro codifica\n",
        "\n",
        "---\n",
        "\n",
        "## âž¡ï¸ Siguiente Notebook\n",
        "\n",
        "ContinÃºa con **`03_langgraph_streaming.ipynb`** para aprender a enviar progreso en tiempo real al frontend.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "codificacion_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

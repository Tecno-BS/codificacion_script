{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Grafo V2: Prompts Divididos\n",
    "\n",
    "Este notebook implementa **una versi√≥n avanzada del grafo** que divide el prompt monol√≠tico en m√∫ltiples nodos especializados.\n",
    "\n",
    "## üéØ Objetivo\n",
    "\n",
    "Tener **m√°xima observabilidad y control** dividiendo el proceso de codificaci√≥n en:\n",
    "\n",
    "1. **Validar** ‚Üí Filtrar respuestas inv√°lidas\n",
    "2. **Buscar en Cat√°logo** ‚Üí Match con c√≥digos hist√≥ricos\n",
    "3. **Generar Nuevos** ‚Üí Crear c√≥digos para lo no cubierto\n",
    "4. **Justificar** ‚Üí Explicar las decisiones\n",
    "5. **Ensamblar** ‚Üí Combinar resultados\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Comparaci√≥n V1 vs V2\n",
    "\n",
    "| Aspecto | V1 (Simple) | V2 (Dividido) |\n",
    "|---------|-------------|---------------|\n",
    "| **Llamadas GPT/batch** | 1 | 4 |\n",
    "| **Observabilidad** | ‚ùå Baja | ‚úÖ Alta |\n",
    "| **Modificabilidad** | ‚ö†Ô∏è Media | ‚úÖ Alta |\n",
    "| **Costo** | ‚úÖ Menor | ‚ö†Ô∏è Mayor |\n",
    "| **Control granular** | ‚ùå No | ‚úÖ S√≠ |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Cu√°ndo Usar V2\n",
    "\n",
    "- ‚úÖ Necesitas entender **exactamente** qu√© hace cada paso\n",
    "- ‚úÖ Quieres **ajustar** solo una parte del proceso\n",
    "- ‚úÖ Est√°s **experimentando** con diferentes estrategias\n",
    "- ‚úÖ Necesitas **auditor√≠a** detallada\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Prerrequisitos\n",
    "\n",
    "Este notebook **extiende** el notebook 03. Puedes:\n",
    "\n",
    "1. **Opci√≥n A:** Ejecutar las celdas 1-7 del notebook 03 primero\n",
    "2. **Opci√≥n B:** Copiar las celdas de setup aqu√≠\n",
    "\n",
    "**Necesitas:**\n",
    "- ‚úÖ Respuestas cargadas (`respuestas_reales`)\n",
    "- ‚úÖ Cat√°logo hist√≥rico (`catalogo_historico`)\n",
    "- ‚úÖ Configuraci√≥n (`MODELO_GPT`, `BATCH_SIZE`, etc.)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üì¶ REUTILIZAR SETUP DEL NOTEBOOK 03\n",
    "# ========================================\n",
    "\n",
    "# Opci√≥n 1: Ejecutar el notebook 03 completo\n",
    "%run 03_experimentacion_real.ipynb\n",
    "\n",
    "print(\"‚úÖ Setup del notebook 03 cargado\")\n",
    "print(\"‚úÖ Variables disponibles: respuestas_reales, catalogo_historico, etc.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî¨ Paso 1: Definir Esquemas Pydantic Especializados\n",
    "\n",
    "Cada nodo tiene su propio esquema de salida:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üì¶ ESQUEMAS PYDANTIC V2\n",
    "# ========================================\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# 1Ô∏è‚É£ Validaci√≥n\n",
    "class ValidacionRespuesta(BaseModel):\n",
    "    respuesta_id: int = Field(description=\"ID de la respuesta (1-based)\")\n",
    "    es_valida: bool = Field(description=\"True si la respuesta tiene contenido √∫til\")\n",
    "    razon: str = Field(description=\"Breve raz√≥n de por qu√© es v√°lida o no\")\n",
    "\n",
    "class ResultadoValidacion(BaseModel):\n",
    "    validaciones: List[ValidacionRespuesta]\n",
    "\n",
    "# 2Ô∏è‚É£ B√∫squeda en Cat√°logo\n",
    "class CodigoHistoricoMatch(BaseModel):\n",
    "    codigo: int = Field(description=\"C√≥digo del cat√°logo hist√≥rico\")\n",
    "    relevancia: float = Field(description=\"Qu√© tan relevante es (0.0-1.0)\", ge=0.0, le=1.0)\n",
    "\n",
    "class BusquedaCatalogo(BaseModel):\n",
    "    respuesta_id: int\n",
    "    codigos_historicos: List[CodigoHistoricoMatch] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"C√≥digos hist√≥ricos que aplican\"\n",
    "    )\n",
    "\n",
    "class ResultadoBusqueda(BaseModel):\n",
    "    busquedas: List[BusquedaCatalogo]\n",
    "\n",
    "# 3Ô∏è‚É£ Generaci√≥n de Nuevos C√≥digos\n",
    "class CodigoNuevo(BaseModel):\n",
    "    descripcion: str = Field(description=\"Descripci√≥n espec√≠fica del concepto\")\n",
    "\n",
    "class GeneracionNuevos(BaseModel):\n",
    "    respuesta_id: int\n",
    "    codigos_nuevos: List[CodigoNuevo] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"C√≥digos nuevos a crear\"\n",
    "    )\n",
    "\n",
    "class ResultadoGeneracion(BaseModel):\n",
    "    generaciones: List[GeneracionNuevos]\n",
    "\n",
    "# 4Ô∏è‚É£ Justificaci√≥n\n",
    "class Justificacion(BaseModel):\n",
    "    respuesta_id: int\n",
    "    justificacion: str = Field(description=\"Explicaci√≥n de la codificaci√≥n\")\n",
    "\n",
    "class ResultadoJustificacion(BaseModel):\n",
    "    justificaciones: List[Justificacion]\n",
    "\n",
    "print(\"‚úÖ Esquemas Pydantic V2 definidos\")\n",
    "print(f\"   ‚Ä¢ ValidacionRespuesta\")\n",
    "print(f\"   ‚Ä¢ BusquedaCatalogo\")\n",
    "print(f\"   ‚Ä¢ GeneracionNuevos\")\n",
    "print(f\"   ‚Ä¢ Justificacion\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèóÔ∏è Paso 2: Definir Nodos del Grafo V2\n",
    "\n",
    "Implementaci√≥n de los 5 nodos especializados:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üèóÔ∏è NODOS DEL GRAFO V2\n",
    "# ========================================\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# NOTA: Reutilizamos nodos del notebook 03 que no cambian:\n",
    "# - nodo_inicializar\n",
    "# - nodo_preparar_batch  \n",
    "# - nodo_incrementar_batch\n",
    "# - nodo_normalizar\n",
    "# - nodo_finalizar\n",
    "# - decidir_continuar\n",
    "\n",
    "# Solo definimos los nodos NUEVOS especializados:\n",
    "\n",
    "def nodo_validar_respuestas_v2(state: EstadoCodificacion) -> EstadoCodificacion:\n",
    "    \"\"\"Paso 1: Valida qu√© respuestas son √∫tiles\"\"\"\n",
    "    print(f\"\\n‚úÖ Validando {len(state['batch_respuestas'])} respuestas...\")\n",
    "    \n",
    "    respuestas_str = \"\\n\".join([\n",
    "        f\"{i+1}. {r['texto']}\"\n",
    "        for i, r in enumerate(state[\"batch_respuestas\"])\n",
    "    ])\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Eres un experto en filtrar respuestas de encuestas.\n",
    "\n",
    "RECHAZAR si:\n",
    "- Est√° vac√≠a o solo tiene \"-\" o \".\"\n",
    "- Es incomprensible\n",
    "- No responde a la pregunta\n",
    "\n",
    "ACEPTAR si tiene contenido relevante.\n",
    "\n",
    "Responde en formato JSON.\"\"\"),\n",
    "        (\"user\", \"PREGUNTA: {pregunta}\\n\\nRESPUESTAS:\\n{respuestas}\")\n",
    "    ])\n",
    "    \n",
    "    llm = ChatOpenAI(model=state[\"modelo_gpt\"], temperature=0)\n",
    "    chain = prompt | llm.with_structured_output(ResultadoValidacion)\n",
    "    \n",
    "    resultado = chain.invoke({\n",
    "        \"pregunta\": state[\"pregunta\"],\n",
    "        \"respuestas\": respuestas_str\n",
    "    })\n",
    "    \n",
    "    validas = sum(1 for v in resultado.validaciones if v.es_valida)\n",
    "    print(f\"   ‚úÖ V√°lidas: {validas}/{len(resultado.validaciones)}\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"validaciones_batch\": [v.model_dump() for v in resultado.validaciones]\n",
    "    }\n",
    "\n",
    "\n",
    "def nodo_buscar_en_catalogo_v2(state: EstadoCodificacion) -> EstadoCodificacion:\n",
    "    \"\"\"Paso 2: Busca c√≥digos hist√≥ricos relevantes\"\"\"\n",
    "    \n",
    "    # Solo procesar las v√°lidas\n",
    "    validas = [\n",
    "        (i, resp) \n",
    "        for i, (resp, val) in enumerate(zip(\n",
    "            state[\"batch_respuestas\"],\n",
    "            state[\"validaciones_batch\"]\n",
    "        ))\n",
    "        if val[\"es_valida\"]\n",
    "    ]\n",
    "    \n",
    "    if not validas or not state[\"catalogo\"]:\n",
    "        print(f\"   ‚ö†Ô∏è  Sin respuestas v√°lidas o sin cat√°logo\")\n",
    "        return {**state, \"busquedas_batch\": []}\n",
    "    \n",
    "    print(f\"\\nüîç Buscando c√≥digos hist√≥ricos para {len(validas)} respuestas...\")\n",
    "    \n",
    "    respuestas_validas_str = \"\\n\".join([\n",
    "        f\"{idx+1}. {resp['texto']}\"\n",
    "        for idx, resp in validas\n",
    "    ])\n",
    "    \n",
    "    catalogo_str = \"\\n\".join([\n",
    "        f\"- C√≥digo {c['codigo']}: {c['descripcion']}\"\n",
    "        for c in state[\"catalogo\"][:30]\n",
    "    ])\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"\"\"Eres un experto en matching sem√°ntico.\n",
    "\n",
    "CAT√ÅLOGO:\n",
    "{catalogo_str}\n",
    "\n",
    "Identifica c√≥digos del cat√°logo relevantes para cada respuesta.\n",
    "- Relevancia de 0.0 a 1.0\n",
    "- Solo incluye si relevancia >= 0.7\n",
    "- Puedes asignar m√∫ltiples c√≥digos\n",
    "\n",
    "Responde en JSON.\"\"\"),\n",
    "        (\"user\", \"PREGUNTA: {pregunta}\\n\\nRESPUESTAS:\\n{respuestas}\")\n",
    "    ])\n",
    "    \n",
    "    llm = ChatOpenAI(model=state[\"modelo_gpt\"], temperature=0)\n",
    "    chain = prompt | llm.with_structured_output(ResultadoBusqueda)\n",
    "    \n",
    "    resultado = chain.invoke({\n",
    "        \"pregunta\": state[\"pregunta\"],\n",
    "        \"respuestas\": respuestas_validas_str\n",
    "    })\n",
    "    \n",
    "    total_matches = sum(len(b.codigos_historicos) for b in resultado.busquedas)\n",
    "    print(f\"   ‚úÖ Matches encontrados: {total_matches}\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"busquedas_batch\": [b.model_dump() for b in resultado.busquedas]\n",
    "    }\n",
    "\n",
    "\n",
    "def nodo_generar_nuevos_v2(state: EstadoCodificacion) -> EstadoCodificacion:\n",
    "    \"\"\"Paso 3: Genera c√≥digos nuevos para lo no cubierto\"\"\"\n",
    "    \n",
    "    respuestas_sin_match = []\n",
    "    \n",
    "    for i, (resp, val) in enumerate(zip(\n",
    "        state[\"batch_respuestas\"],\n",
    "        state[\"validaciones_batch\"]\n",
    "    )):\n",
    "        if not val[\"es_valida\"]:\n",
    "            continue\n",
    "        \n",
    "        busqueda = next(\n",
    "            (b for b in state[\"busquedas_batch\"] if b[\"respuesta_id\"] == i+1),\n",
    "            None\n",
    "        )\n",
    "        \n",
    "        if not busqueda or len(busqueda[\"codigos_historicos\"]) == 0:\n",
    "            respuestas_sin_match.append((i+1, resp[\"texto\"]))\n",
    "    \n",
    "    if not respuestas_sin_match:\n",
    "        print(f\"   ‚úÖ Todas cubiertas con c√≥digos hist√≥ricos\")\n",
    "        return {**state, \"generaciones_batch\": []}\n",
    "    \n",
    "    print(f\"\\nüÜï Generando c√≥digos nuevos para {len(respuestas_sin_match)} respuestas...\")\n",
    "    \n",
    "    respuestas_str = \"\\n\".join([\n",
    "        f\"{resp_id}. {texto}\"\n",
    "        for resp_id, texto in respuestas_sin_match\n",
    "    ])\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Eres un experto en crear c√≥digos de categorizaci√≥n.\n",
    "\n",
    "REGLAS:\n",
    "1. Descripciones ESPEC√çFICAS:\n",
    "   ‚úÖ \"Versatilidad de uso\"\n",
    "   ‚ùå \"Versatilidad\" (muy gen√©rico)\n",
    "\n",
    "2. UNIFICA conceptos similares\n",
    "3. Puedes asignar M√öLTIPLES c√≥digos\n",
    "4. S√© CONSISTENTE\n",
    "\n",
    "Responde en JSON.\"\"\"),\n",
    "        (\"user\", \"PREGUNTA: {pregunta}\\n\\nRESPUESTAS:\\n{respuestas}\")\n",
    "    ])\n",
    "    \n",
    "    llm = ChatOpenAI(model=state[\"modelo_gpt\"], temperature=0)\n",
    "    chain = prompt | llm.with_structured_output(ResultadoGeneracion)\n",
    "    \n",
    "    resultado = chain.invoke({\n",
    "        \"pregunta\": state[\"pregunta\"],\n",
    "        \"respuestas\": respuestas_str\n",
    "    })\n",
    "    \n",
    "    total_nuevos = sum(len(g.codigos_nuevos) for g in resultado.generaciones)\n",
    "    print(f\"   ‚úÖ C√≥digos nuevos: {total_nuevos}\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"generaciones_batch\": [g.model_dump() for g in resultado.generaciones]\n",
    "    }\n",
    "\n",
    "\n",
    "def nodo_justificar_v2(state: EstadoCodificacion) -> EstadoCodificacion:\n",
    "    \"\"\"Paso 4: Genera justificaciones\"\"\"\n",
    "    print(f\"\\nüìù Generando justificaciones...\")\n",
    "    \n",
    "    resumen_decisiones = []\n",
    "    \n",
    "    for i, (resp, val) in enumerate(zip(\n",
    "        state[\"batch_respuestas\"],\n",
    "        state[\"validaciones_batch\"]\n",
    "    )):\n",
    "        resp_id = i + 1\n",
    "        \n",
    "        if not val[\"es_valida\"]:\n",
    "            resumen_decisiones.append(\n",
    "                f\"{resp_id}. RECHAZADA: {val['razon']}\"\n",
    "            )\n",
    "            continue\n",
    "        \n",
    "        busqueda = next(\n",
    "            (b for b in state[\"busquedas_batch\"] if b[\"respuesta_id\"] == resp_id),\n",
    "            {\"codigos_historicos\": []}\n",
    "        )\n",
    "        \n",
    "        generacion = next(\n",
    "            (g for g in state[\"generaciones_batch\"] if g[\"respuesta_id\"] == resp_id),\n",
    "            {\"codigos_nuevos\": []}\n",
    "        )\n",
    "        \n",
    "        resumen_decisiones.append(\n",
    "            f\"{resp_id}. Hist:{len(busqueda.get('codigos_historicos', []))} \"\n",
    "            f\"Nuevos:{len(generacion.get('codigos_nuevos', []))}\"\n",
    "        )\n",
    "    \n",
    "    resumen_str = \"\\n\".join(resumen_decisiones)\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"Genera justificaciones BREVES (1-2 oraciones) explicando:\n",
    "- Por qu√© se rechaz√≥\n",
    "- Por qu√© se usaron c√≥digos hist√≥ricos\n",
    "- Por qu√© se crearon nuevos\n",
    "\n",
    "S√© CONCISO. Responde en JSON.\"\"\"),\n",
    "        (\"user\", \"DECISIONES:\\n{resumen}\")\n",
    "    ])\n",
    "    \n",
    "    llm = ChatOpenAI(model=state[\"modelo_gpt\"], temperature=0)\n",
    "    chain = prompt | llm.with_structured_output(ResultadoJustificacion)\n",
    "    \n",
    "    resultado = chain.invoke({\"resumen\": resumen_str})\n",
    "    \n",
    "    print(f\"   ‚úÖ Justificaciones generadas\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"justificaciones_batch\": [j.model_dump() for j in resultado.justificaciones]\n",
    "    }\n",
    "\n",
    "\n",
    "def nodo_ensamblar_resultados_v2(state: EstadoCodificacion) -> EstadoCodificacion:\n",
    "    \"\"\"Paso 5: Ensambla resultados finales\"\"\"\n",
    "    print(f\"\\nüîß Ensamblando resultados...\")\n",
    "    \n",
    "    codificaciones_batch = []\n",
    "    \n",
    "    for i, (resp, val) in enumerate(zip(\n",
    "        state[\"batch_respuestas\"],\n",
    "        state[\"validaciones_batch\"]\n",
    "    )):\n",
    "        resp_id = i + 1\n",
    "        \n",
    "        justif = next(\n",
    "            (j for j in state[\"justificaciones_batch\"] if j[\"respuesta_id\"] == resp_id),\n",
    "            {\"justificacion\": \"Sin justificaci√≥n\"}\n",
    "        )\n",
    "        \n",
    "        if not val[\"es_valida\"]:\n",
    "            codificaciones_batch.append({\n",
    "                \"fila_excel\": resp[\"fila_excel\"],\n",
    "                \"texto\": resp[\"texto\"],\n",
    "                \"decision\": \"rechazar\",\n",
    "                \"codigos_historicos\": [],\n",
    "                \"codigos_nuevos\": [],\n",
    "                \"justificacion\": justif[\"justificacion\"]\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        busqueda = next(\n",
    "            (b for b in state[\"busquedas_batch\"] if b[\"respuesta_id\"] == resp_id),\n",
    "            {\"codigos_historicos\": []}\n",
    "        )\n",
    "        \n",
    "        generacion = next(\n",
    "            (g for g in state[\"generaciones_batch\"] if g[\"respuesta_id\"] == resp_id),\n",
    "            {\"codigos_nuevos\": []}\n",
    "        )\n",
    "        \n",
    "        codigos_hist = [c[\"codigo\"] for c in busqueda.get(\"codigos_historicos\", [])]\n",
    "        codigos_nuevos = [c[\"descripcion\"] for c in generacion.get(\"codigos_nuevos\", [])]\n",
    "        \n",
    "        if codigos_hist and codigos_nuevos:\n",
    "            decision = \"mixto\"\n",
    "        elif codigos_hist:\n",
    "            decision = \"historico\"\n",
    "        elif codigos_nuevos:\n",
    "            decision = \"nuevo\"\n",
    "        else:\n",
    "            decision = \"rechazar\"\n",
    "        \n",
    "        codificaciones_batch.append({\n",
    "            \"fila_excel\": resp[\"fila_excel\"],\n",
    "            \"texto\": resp[\"texto\"],\n",
    "            \"decision\": decision,\n",
    "            \"codigos_historicos\": codigos_hist,\n",
    "            \"codigos_nuevos\": codigos_nuevos,\n",
    "            \"justificacion\": justif[\"justificacion\"]\n",
    "        })\n",
    "    \n",
    "    decisiones = {}\n",
    "    for cod in codificaciones_batch:\n",
    "        dec = cod[\"decision\"]\n",
    "        decisiones[dec] = decisiones.get(dec, 0) + 1\n",
    "    \n",
    "    print(f\"   üìä Decisiones: {decisiones}\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"codificaciones\": state[\"codificaciones\"] + codificaciones_batch,\n",
    "        \"mensaje_estado\": f\"Batch {state['batch_actual'] + 1} completado (V2)\"\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Nodos V2 definidos:\")\n",
    "print(\"   1. nodo_validar_respuestas_v2\")\n",
    "print(\"   2. nodo_buscar_en_catalogo_v2\")\n",
    "print(\"   3. nodo_generar_nuevos_v2\")\n",
    "print(\"   4. nodo_justificar_v2\")\n",
    "print(\"   5. nodo_ensamblar_resultados_v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó Paso 3: Construir el Grafo V2\n",
    "\n",
    "Conectar todos los nodos en el flujo especializado:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üîó CONSTRUIR GRAFO V2\n",
    "# ========================================\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List, Dict\n",
    "\n",
    "# Extender el estado para incluir campos de V2\n",
    "class EstadoCodificacionV2(TypedDict):\n",
    "    # Campos originales (del notebook 03)\n",
    "    pregunta: str\n",
    "    modelo_gpt: str\n",
    "    batch_size: int\n",
    "    respuestas: List[Dict]\n",
    "    catalogo: List[Dict]\n",
    "    batch_actual: int\n",
    "    total_batches: int\n",
    "    batch_respuestas: List[Dict]\n",
    "    codificaciones: List[Dict]\n",
    "    codigos_nuevos_global: Dict[str, str]\n",
    "    contador_codigo_nuevo: int\n",
    "    costo_total: float\n",
    "    tokens_total: int\n",
    "    mensaje_estado: str\n",
    "    progreso_pct: float\n",
    "    tiempo_inicio: float\n",
    "    tiempo_total: float\n",
    "    \n",
    "    # üÜï Campos nuevos para V2\n",
    "    validaciones_batch: List[Dict]\n",
    "    busquedas_batch: List[Dict]\n",
    "    generaciones_batch: List[Dict]\n",
    "    justificaciones_batch: List[Dict]\n",
    "\n",
    "\n",
    "# Construir el grafo V2\n",
    "workflow_v2 = StateGraph(EstadoCodificacionV2)\n",
    "\n",
    "# Agregar nodos (reutilizamos los del notebook 03 cuando sea posible)\n",
    "workflow_v2.add_node(\"inicializar\", nodo_inicializar)\n",
    "workflow_v2.add_node(\"preparar_batch\", nodo_preparar_batch)\n",
    "\n",
    "# üÜï Nodos especializados de V2\n",
    "workflow_v2.add_node(\"validar\", nodo_validar_respuestas_v2)\n",
    "workflow_v2.add_node(\"buscar_catalogo\", nodo_buscar_en_catalogo_v2)\n",
    "workflow_v2.add_node(\"generar_nuevos\", nodo_generar_nuevos_v2)\n",
    "workflow_v2.add_node(\"justificar\", nodo_justificar_v2)\n",
    "workflow_v2.add_node(\"ensamblar\", nodo_ensamblar_resultados_v2)\n",
    "\n",
    "workflow_v2.add_node(\"incrementar\", nodo_incrementar_batch)\n",
    "workflow_v2.add_node(\"normalizar\", nodo_normalizar)\n",
    "workflow_v2.add_node(\"finalizar\", nodo_finalizar)\n",
    "\n",
    "# Conectar flujo\n",
    "workflow_v2.set_entry_point(\"inicializar\")\n",
    "workflow_v2.add_edge(\"inicializar\", \"preparar_batch\")\n",
    "\n",
    "# üîÑ Flujo de codificaci√≥n dividido en 5 pasos\n",
    "workflow_v2.add_edge(\"preparar_batch\", \"validar\")\n",
    "workflow_v2.add_edge(\"validar\", \"buscar_catalogo\")\n",
    "workflow_v2.add_edge(\"buscar_catalogo\", \"generar_nuevos\")\n",
    "workflow_v2.add_edge(\"generar_nuevos\", \"justificar\")\n",
    "workflow_v2.add_edge(\"justificar\", \"ensamblar\")\n",
    "\n",
    "# Continuar con siguiente batch o normalizar\n",
    "workflow_v2.add_edge(\"ensamblar\", \"incrementar\")\n",
    "\n",
    "workflow_v2.add_conditional_edges(\n",
    "    \"incrementar\",\n",
    "    decidir_continuar,\n",
    "    {\n",
    "        \"preparar_batch\": \"preparar_batch\",\n",
    "        \"normalizar\": \"normalizar\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow_v2.add_edge(\"normalizar\", \"finalizar\")\n",
    "workflow_v2.add_edge(\"finalizar\", END)\n",
    "\n",
    "# Compilar\n",
    "app_v2 = workflow_v2.compile()\n",
    "\n",
    "print(\"‚úÖ Grafo V2 compilado exitosamente\")\n",
    "print(\"\\nüìä Flujo del Grafo V2:\")\n",
    "print(\"   inicializar ‚Üí preparar_batch\")\n",
    "print(\"   ‚Üì\")\n",
    "print(\"   validar ‚Üí buscar_catalogo ‚Üí generar_nuevos ‚Üí justificar ‚Üí ensamblar\")\n",
    "print(\"   ‚Üì\")\n",
    "print(\"   incrementar ‚Üí [preparar_batch √≥ normalizar]\")\n",
    "print(\"   ‚Üì\")\n",
    "print(\"   finalizar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Paso 4: Visualizar el Grafo V2\n",
    "\n",
    "Ahora veamos c√≥mo se ve la estructura del grafo:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üìä VISUALIZAR GRAFO V2\n",
    "# ========================================\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä VISUALIZACI√ìN DEL GRAFO V2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Obtener info del grafo\n",
    "graph_v2 = app_v2.get_graph()\n",
    "\n",
    "print(f\"\\nüìã Informaci√≥n:\")\n",
    "print(f\"   Total de nodos: {len(graph_v2.nodes)}\")\n",
    "\n",
    "# Lista de nodos\n",
    "print(f\"\\nüîπ Nodos del grafo:\")\n",
    "for i, node_id in enumerate(graph_v2.nodes.keys(), 1):\n",
    "    print(f\"   {i}. {node_id}\")\n",
    "\n",
    "# Vista ASCII\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìä DIAGRAMA ASCII\")\n",
    "print(\"=\"*60)\n",
    "print(graph_v2.draw_ascii())\n",
    "\n",
    "# Intentar generar PNG\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üñºÔ∏è  DIAGRAMA GR√ÅFICO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    img_data = graph_v2.draw_mermaid_png()\n",
    "    display(Image(img_data))\n",
    "    print(\"\\n‚úÖ Imagen generada correctamente\")\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è  No se pudo generar imagen PNG\")\n",
    "    print(f\"   Raz√≥n: {str(e)[:80]}...\")\n",
    "    print(\"\\nüìã C√≥digo Mermaid (pega en https://mermaid.live):\")\n",
    "    print(\"=\"*60)\n",
    "    mermaid_code = graph_v2.draw_mermaid()\n",
    "    print(mermaid_code)\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Guardar en archivo\n",
    "    output_dir = project_root / \"notebooks\" / \"diagramas\"\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    output_file = output_dir / \"grafo_v2.mmd\"\n",
    "    output_file.write_text(mermaid_code, encoding=\"utf-8\")\n",
    "    print(f\"\\nüíæ C√≥digo guardado en: notebooks/diagramas/grafo_v2.mmd\")\n",
    "    print(\"üí° Visita https://mermaid.live y pega el contenido del archivo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üÜö Comparar V1 vs V2\n",
    "\n",
    "Veamos las diferencias estructurales:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üÜö COMPARACI√ìN V1 vs V2\n",
    "# ========================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üÜö COMPARACI√ìN: V1 (Simple) vs V2 (Dividido)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# An√°lisis del grafo V1 (del notebook 03)\n",
    "graph_v1 = app.get_graph()\n",
    "nodos_v1 = len(graph_v1.nodes)\n",
    "nodos_logicos_v1 = sum(1 for n in graph_v1.nodes.keys() if not n.startswith(\"__\"))\n",
    "\n",
    "# An√°lisis del grafo V2\n",
    "nodos_v2 = len(graph_v2.nodes)\n",
    "nodos_logicos_v2 = sum(1 for n in graph_v2.nodes.keys() if not n.startswith(\"__\"))\n",
    "\n",
    "# Mostrar comparaci√≥n\n",
    "print(f\"\\n{'M√©trica':<30} {'V1 (Simple)':<20} {'V2 (Dividido)':<20}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Total de nodos':<30} {nodos_v1:<20} {nodos_v2:<20}\")\n",
    "print(f\"{'Nodos l√≥gicos':<30} {nodos_logicos_v1:<20} {nodos_logicos_v2:<20}\")\n",
    "\n",
    "diferencia = nodos_logicos_v2 - nodos_logicos_v1\n",
    "porcentaje = (diferencia / nodos_logicos_v1) * 100 if nodos_logicos_v1 > 0 else 0\n",
    "\n",
    "print(f\"\\nüìä Diferencia: +{diferencia} nodos l√≥gicos (+{porcentaje:.1f}%)\")\n",
    "\n",
    "# An√°lisis por batch\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìà AN√ÅLISIS DE COMPLEJIDAD POR BATCH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüîπ V1 (Simple):\")\n",
    "print(f\"   ‚Ä¢ Nodos ejecutados por batch: ~5\")\n",
    "print(f\"   ‚Ä¢ Llamadas a GPT por batch: 1\")\n",
    "print(f\"   ‚Ä¢ Observabilidad: ‚ö†Ô∏è  Baja\")\n",
    "\n",
    "print(f\"\\nüîπ V2 (Dividido):\")\n",
    "print(f\"   ‚Ä¢ Nodos ejecutados por batch: ~10\")\n",
    "print(f\"   ‚Ä¢ Llamadas a GPT por batch: 4\")\n",
    "print(f\"   ‚Ä¢ Observabilidad: ‚úÖ Alta\")\n",
    "\n",
    "print(f\"\\nüí∞ IMPACTO EN COSTOS:\")\n",
    "print(f\"   Con 100 respuestas y batch_size=10:\")\n",
    "print(f\"   ‚Ä¢ V1: 10 batches √ó 1 llamada = 10 llamadas GPT\")\n",
    "print(f\"   ‚Ä¢ V2: 10 batches √ó 4 llamadas = 40 llamadas GPT (4x m√°s caro)\")\n",
    "\n",
    "print(f\"\\n‚úÖ RECOMENDACIONES:\")\n",
    "print(f\"   ‚Ä¢ Usa V1 para: Producci√≥n, menor costo, velocidad\")\n",
    "print(f\"   ‚Ä¢ Usa V2 para: Experimentaci√≥n, ajustes finos, auditor√≠a\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Paso 5: Ejecutar con Datos Reales\n",
    "\n",
    "Ahora vamos a procesar tus datos reales con el Grafo V2:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üöÄ PREPARAR ESTADO INICIAL V2\n",
    "# ========================================\n",
    "\n",
    "import time\n",
    "\n",
    "# Estado inicial extendido con campos de V2\n",
    "estado_inicial_v2 = {\n",
    "    \"pregunta\": nombre_pregunta,\n",
    "    \"modelo_gpt\": MODELO_GPT,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"respuestas\": respuestas_reales,\n",
    "    \"catalogo\": catalogo_historico,\n",
    "    \"batch_actual\": 0,\n",
    "    \"total_batches\": 0,\n",
    "    \"batch_respuestas\": [],\n",
    "    \"codificaciones\": [],\n",
    "    \"codigos_nuevos_global\": {},\n",
    "    \"contador_codigo_nuevo\": 1,\n",
    "    \"costo_total\": 0.0,\n",
    "    \"tokens_total\": 0,\n",
    "    \"mensaje_estado\": \"Iniciando\",\n",
    "    \"progreso_pct\": 0.0,\n",
    "    \"tiempo_inicio\": time.time(),\n",
    "    \"tiempo_total\": 0.0,\n",
    "    \n",
    "    # üÜï Campos adicionales para V2\n",
    "    \"validaciones_batch\": [],\n",
    "    \"busquedas_batch\": [],\n",
    "    \"generaciones_batch\": [],\n",
    "    \"justificaciones_batch\": []\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìã ESTADO INICIAL V2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä Configuraci√≥n:\")\n",
    "print(f\"   Pregunta: {nombre_pregunta}\")\n",
    "print(f\"   Total respuestas: {len(respuestas_reales)}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Cat√°logo hist√≥rico: {len(catalogo_historico)} c√≥digos\")\n",
    "print(f\"   Modelo GPT: {MODELO_GPT}\")\n",
    "\n",
    "# Calcular batches esperados\n",
    "batches_esperados = (len(respuestas_reales) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "print(f\"\\nüßÆ C√°lculo de batches:\")\n",
    "print(f\"   {len(respuestas_reales)} respuestas √∑ {BATCH_SIZE} por batch\")\n",
    "print(f\"   = {batches_esperados} batches esperados\")\n",
    "\n",
    "if batches_esperados == 0:\n",
    "    print(f\"\\n‚ùå ERROR: No hay respuestas para procesar!\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Listo para ejecutar con {batches_esperados} batches\")\n",
    "    \n",
    "# Estimar llamadas y costo\n",
    "llamadas_v1 = batches_esperados * 1\n",
    "llamadas_v2 = batches_esperados * 4\n",
    "\n",
    "print(f\"\\nüí∞ Estimaci√≥n de llamadas GPT:\")\n",
    "print(f\"   V1 (Simple): {llamadas_v1} llamadas\")\n",
    "print(f\"   V2 (Dividido): {llamadas_v2} llamadas (4x m√°s)\")\n",
    "print(f\"\\n‚ö†Ô∏è  V2 ser√° aproximadamente 4x m√°s costoso que V1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üöÄ EJECUTAR GRAFO V2 CON DATOS REALES\n",
    "# ========================================\n",
    "\n",
    "from langgraph.pregel.main import RunnableConfig\n",
    "\n",
    "# Calcular l√≠mite de recursi√≥n apropiado\n",
    "# V2 tiene m√°s nodos por batch (~10 vs ~5), necesitamos m√°s margen\n",
    "limite_recursion = max(batches_esperados * 15 + 20, 100)\n",
    "\n",
    "config_v2 = RunnableConfig(\n",
    "    recursion_limit=limite_recursion\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ INICIANDO CODIFICACI√ìN CON GRAFO V2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Configuraci√≥n de ejecuci√≥n:\")\n",
    "print(f\"   L√≠mite de recursi√≥n: {limite_recursion}\")\n",
    "print(f\"   (suficiente para {batches_esperados} batches)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EJECUTANDO GRAFO V2...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # EJECUTAR\n",
    "    resultado_final_v2 = app_v2.invoke(estado_inicial_v2, config=config_v2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ PROCESO V2 COMPLETADO EXITOSAMENTE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Mostrar estad√≠sticas\n",
    "    print(f\"\\nüìä Estad√≠sticas finales:\")\n",
    "    print(f\"   Tiempo total: {resultado_final_v2['tiempo_total']:.1f}s\")\n",
    "    print(f\"   Respuestas procesadas: {len(resultado_final_v2['codificaciones'])}\")\n",
    "    print(f\"   C√≥digos nuevos generados: {len(resultado_final_v2['codigos_nuevos_global'])}\")\n",
    "    \n",
    "    # An√°lisis de decisiones\n",
    "    decisiones_v2 = {}\n",
    "    for cod in resultado_final_v2[\"codificaciones\"]:\n",
    "        dec = cod[\"decision\"]\n",
    "        decisiones_v2[dec] = decisiones_v2.get(dec, 0) + 1\n",
    "    \n",
    "    print(f\"\\nüìà Distribuci√≥n de decisiones:\")\n",
    "    for decision, cantidad in sorted(decisiones_v2.items(), key=lambda x: -x[1]):\n",
    "        porcentaje = (cantidad / len(resultado_final_v2['codificaciones'])) * 100\n",
    "        print(f\"   {decision:12} : {cantidad:3} ({porcentaje:.1f}%)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ùå ERROR DURANTE EJECUCI√ìN V2\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(f\"\\nTipo: {type(e).__name__}\")\n",
    "    \n",
    "    import traceback\n",
    "    print(\"\\nüìã Traceback completo:\")\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Paso 6: Analizar Resultados\n",
    "\n",
    "Veamos qu√© tan diferente es V2 vs V1:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üìä AN√ÅLISIS COMPARATIVO DE RESULTADOS\n",
    "# ========================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìä AN√ÅLISIS: Resultados V1 vs V2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Resultados de V1 (del notebook 03)\n",
    "decisiones_v1 = {}\n",
    "for cod in resultado_final[\"codificaciones\"]:\n",
    "    dec = cod[\"decision\"]\n",
    "    decisiones_v1[dec] = decisiones_v1.get(dec, 0) + 1\n",
    "\n",
    "print(f\"\\nüîπ GRAFO V1 (Simple):\")\n",
    "print(f\"   Tiempo: {resultado_final['tiempo_total']:.1f}s\")\n",
    "print(f\"   Respuestas: {len(resultado_final['codificaciones'])}\")\n",
    "print(f\"   C√≥digos nuevos: {len(resultado_final['codigos_nuevos_global'])}\")\n",
    "print(f\"\\n   Decisiones:\")\n",
    "for decision, cantidad in sorted(decisiones_v1.items(), key=lambda x: -x[1]):\n",
    "    porcentaje = (cantidad / len(resultado_final['codificaciones'])) * 100\n",
    "    print(f\"      {decision:12} : {cantidad:3} ({porcentaje:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüîπ GRAFO V2 (Dividido):\")\n",
    "print(f\"   Tiempo: {resultado_final_v2['tiempo_total']:.1f}s\")\n",
    "print(f\"   Respuestas: {len(resultado_final_v2['codificaciones'])}\")\n",
    "print(f\"   C√≥digos nuevos: {len(resultado_final_v2['codigos_nuevos_global'])}\")\n",
    "print(f\"\\n   Decisiones:\")\n",
    "for decision, cantidad in sorted(decisiones_v2.items(), key=lambda x: -x[1]):\n",
    "    porcentaje = (cantidad / len(resultado_final_v2['codificaciones'])) * 100\n",
    "    print(f\"      {decision:12} : {cantidad:3} ({porcentaje:.1f}%)\")\n",
    "\n",
    "# Comparaci√≥n\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üÜö DIFERENCIAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "diferencia_tiempo = resultado_final_v2['tiempo_total'] - resultado_final['tiempo_total']\n",
    "diferencia_pct = (diferencia_tiempo / resultado_final['tiempo_total']) * 100 if resultado_final['tiempo_total'] > 0 else 0\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Tiempo:\")\n",
    "print(f\"   V2 tom√≥ {abs(diferencia_tiempo):.1f}s {'m√°s' if diferencia_tiempo > 0 else 'menos'}\")\n",
    "print(f\"   ({abs(diferencia_pct):.1f}% {'m√°s lento' if diferencia_tiempo > 0 else 'm√°s r√°pido'})\")\n",
    "\n",
    "diferencia_codigos = len(resultado_final_v2['codigos_nuevos_global']) - len(resultado_final['codigos_nuevos_global'])\n",
    "print(f\"\\nüÜï C√≥digos nuevos:\")\n",
    "print(f\"   V2 gener√≥ {abs(diferencia_codigos)} c√≥digos {'m√°s' if diferencia_codigos > 0 else 'menos'} que V1\")\n",
    "\n",
    "print(f\"\\nüí° OBSERVACIONES:\")\n",
    "if diferencia_tiempo > 0:\n",
    "    print(f\"   ‚Ä¢ V2 es m√°s lento debido a 4x m√°s llamadas GPT\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Tiempos similares (las llamadas fueron r√°pidas)\")\n",
    "\n",
    "if diferencia_codigos != 0:\n",
    "    print(f\"   ‚Ä¢ V2 gener√≥ diferentes c√≥digos (prompts m√°s espec√≠ficos)\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Ambos generaron la misma cantidad de c√≥digos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì• Paso 7: Exportar Resultados\n",
    "\n",
    "Guardar los resultados de V2 para an√°lisis posterior:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üì• EXPORTAR RESULTADOS V2\n",
    "# ========================================\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Preparar DataFrame con resultados de V2\n",
    "datos_exportar_v2 = []\n",
    "\n",
    "for cod in resultado_final_v2[\"codificaciones\"]:\n",
    "    # Preparar c√≥digos como strings\n",
    "    if cod[\"decision\"] == \"rechazar\":\n",
    "        codigo_final = \"RECHAZADO\"\n",
    "    else:\n",
    "        codigos = []\n",
    "        # C√≥digos hist√≥ricos\n",
    "        if cod[\"codigos_historicos\"]:\n",
    "            codigos.extend([str(c) for c in cod[\"codigos_historicos\"]])\n",
    "        # C√≥digos nuevos (buscar sus n√∫meros)\n",
    "        if cod[\"codigos_nuevos\"]:\n",
    "            for desc in cod[\"codigos_nuevos\"]:\n",
    "                # Buscar el c√≥digo num√©rico asignado\n",
    "                codigo_num = next(\n",
    "                    (k for k, v in resultado_final_v2[\"codigos_nuevos_global\"].items() if v == desc),\n",
    "                    desc  # Si no se encuentra, usar la descripci√≥n\n",
    "                )\n",
    "                codigos.append(str(codigo_num))\n",
    "        \n",
    "        codigo_final = \"; \".join(codigos) if codigos else \"SIN_CODIGO\"\n",
    "    \n",
    "    datos_exportar_v2.append({\n",
    "        \"Respuesta\": cod[\"texto\"],\n",
    "        \"C√≥digo\": codigo_final,\n",
    "        \"Decisi√≥n\": cod[\"decision\"],\n",
    "        \"Justificaci√≥n\": cod.get(\"justificacion\", \"\")\n",
    "    })\n",
    "\n",
    "df_resultados_v2 = pd.DataFrame(datos_exportar_v2)\n",
    "\n",
    "# Generar nombre de archivo\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "nombre_archivo_base = ARCHIVO_RESPUESTAS.stem\n",
    "modelo_limpio = MODELO_GPT.replace(\"-\", \"_\").replace(\".\", \"_\")\n",
    "\n",
    "archivo_salida_v2 = project_root / \"notebooks\" / \"resultados\" / f\"{nombre_archivo_base}_v2_{modelo_limpio}_{timestamp}.xlsx\"\n",
    "archivo_salida_v2.parent.mkdir(exist_ok=True)\n",
    "\n",
    "# Guardar\n",
    "df_resultados_v2.to_excel(archivo_salida_v2, index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üì• EXPORTACI√ìN COMPLETADA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n‚úÖ Archivo guardado en:\")\n",
    "print(f\"   {archivo_salida_v2.relative_to(project_root)}\")\n",
    "\n",
    "print(f\"\\nüìä Contenido:\")\n",
    "print(f\"   Filas: {len(df_resultados_v2)}\")\n",
    "print(f\"   Columnas: {list(df_resultados_v2.columns)}\")\n",
    "\n",
    "print(f\"\\nüìã Vista previa (primeras 3 filas):\")\n",
    "print(df_resultados_v2.head(3).to_string())\n",
    "\n",
    "# Tambi√©n guardar cat√°logo de c√≥digos nuevos si hay\n",
    "if resultado_final_v2[\"codigos_nuevos_global\"]:\n",
    "    archivo_catalogo_v2 = project_root / \"notebooks\" / \"resultados\" / f\"catalogo_nuevos_v2_{timestamp}.xlsx\"\n",
    "    \n",
    "    df_catalogo_v2 = pd.DataFrame([\n",
    "        {\"C√≥digo\": cod, \"Descripci√≥n\": desc}\n",
    "        for cod, desc in resultado_final_v2[\"codigos_nuevos_global\"].items()\n",
    "    ])\n",
    "    \n",
    "    df_catalogo_v2.to_excel(archivo_catalogo_v2, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Cat√°logo de c√≥digos nuevos guardado en:\")\n",
    "    print(f\"   {archivo_catalogo_v2.relative_to(project_root)}\")\n",
    "    print(f\"   Total c√≥digos: {len(df_catalogo_v2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Inspecci√≥n Detallada (Opcional)\n",
    "\n",
    "Veamos c√≥mo funcion√≥ cada nodo en detalle para un batch espec√≠fico:\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# üîç INSPECCI√ìN DETALLADA (EJEMPLOS)\n",
    "# ========================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üîç INSPECCI√ìN: Ejemplos de Resultados V2\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Mostrar algunos ejemplos de cada tipo de decisi√≥n\n",
    "print(\"\\nüìã Ejemplos por tipo de decisi√≥n:\\n\")\n",
    "\n",
    "for tipo_decision in [\"historico\", \"nuevo\", \"mixto\", \"rechazar\"]:\n",
    "    ejemplos = [\n",
    "        cod for cod in resultado_final_v2[\"codificaciones\"]\n",
    "        if cod[\"decision\"] == tipo_decision\n",
    "    ][:2]  # Solo primeros 2 de cada tipo\n",
    "    \n",
    "    if ejemplos:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üîπ {tipo_decision.upper()}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for i, ej in enumerate(ejemplos, 1):\n",
    "            print(f\"\\n{i}. Respuesta: \\\"{ej['texto'][:80]}...\\\"\")\n",
    "            \n",
    "            if ej[\"decision\"] != \"rechazar\":\n",
    "                if ej[\"codigos_historicos\"]:\n",
    "                    print(f\"   üìö C√≥digos hist√≥ricos: {ej['codigos_historicos']}\")\n",
    "                if ej[\"codigos_nuevos\"]:\n",
    "                    print(f\"   üÜï C√≥digos nuevos: {ej['codigos_nuevos']}\")\n",
    "            \n",
    "            if ej.get(\"justificacion\"):\n",
    "                print(f\"   üí¨ Justificaci√≥n: {ej['justificacion']}\")\n",
    "\n",
    "# Mostrar c√≥digos nuevos m√°s comunes (si hay)\n",
    "if resultado_final_v2[\"codigos_nuevos_global\"]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üÜï CAT√ÅLOGO DE C√ìDIGOS NUEVOS GENERADOS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Contar uso de cada c√≥digo\n",
    "    uso_codigos = {}\n",
    "    for cod in resultado_final_v2[\"codificaciones\"]:\n",
    "        for desc in cod[\"codigos_nuevos\"]:\n",
    "            uso_codigos[desc] = uso_codigos.get(desc, 0) + 1\n",
    "    \n",
    "    # Top 10 m√°s usados\n",
    "    top_codigos = sorted(uso_codigos.items(), key=lambda x: -x[1])[:10]\n",
    "    \n",
    "    print(f\"\\nüìä Top 10 c√≥digos nuevos m√°s usados:\\n\")\n",
    "    for i, (desc, cantidad) in enumerate(top_codigos, 1):\n",
    "        codigo_num = next(\n",
    "            (k for k, v in resultado_final_v2[\"codigos_nuevos_global\"].items() if v == desc),\n",
    "            \"?\"\n",
    "        )\n",
    "        print(f\"{i:2}. [{codigo_num:4}] {desc[:50]:50} (usado {cantidad} veces)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"‚úÖ Inspecci√≥n completada\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Resumen Final\n",
    "\n",
    "### **üéØ Qu√© Lograste:**\n",
    "\n",
    "1. ‚úÖ Ejecutaste el **Grafo V2** con prompts divididos\n",
    "2. ‚úÖ Procesaste tus **datos reales**\n",
    "3. ‚úÖ Comparaste **V1 vs V2** en calidad y tiempo\n",
    "4. ‚úÖ Exportaste **resultados** a Excel\n",
    "5. ‚úÖ Analizaste **c√≥digos nuevos** generados\n",
    "\n",
    "---\n",
    "\n",
    "### **üìä Decisi√≥n: ¬øV1 o V2?**\n",
    "\n",
    "#### **Usa V1 (Notebook 03) si:**\n",
    "- ‚úÖ Los resultados son suficientemente buenos\n",
    "- ‚úÖ Quieres **menor costo** (1 llamada vs 4 por batch)\n",
    "- ‚úÖ Velocidad es importante\n",
    "- ‚úÖ Vas a **producci√≥n**\n",
    "\n",
    "#### **Usa V2 (Este notebook) si:**\n",
    "- ‚úÖ Necesitas **ajustar** validaci√≥n/b√∫squeda/generaci√≥n por separado\n",
    "- ‚úÖ Quieres **m√°xima observabilidad**\n",
    "- ‚úÖ Est√°s **experimentando** con prompts\n",
    "- ‚úÖ Necesitas **justificaciones** detalladas\n",
    "- ‚úÖ El costo extra vale la pena\n",
    "\n",
    "---\n",
    "\n",
    "### **üöÄ Pr√≥ximos Pasos:**\n",
    "\n",
    "1. **Revisar resultados** en los archivos Excel generados\n",
    "2. **Comparar manualmente** 10-20 ejemplos de V1 vs V2\n",
    "3. **Ajustar prompts** en nodos espec√≠ficos si es necesario\n",
    "4. **Decidir arquitectura** para llevar a producci√≥n\n",
    "5. **Migrar c√≥digo** al backend cuando est√© listo\n",
    "\n",
    "---\n",
    "\n",
    "### **üí° Tips para Optimizar V2:**\n",
    "\n",
    "1. **Modelos diferentes por etapa:**\n",
    "   ```python\n",
    "   # En validar y generar: usa gpt-4o (preciso)\n",
    "   # En buscar y justificar: usa gpt-3.5-turbo (barato)\n",
    "   ```\n",
    "\n",
    "2. **Ajustar umbrales:**\n",
    "   ```python\n",
    "   # En buscar_catalogo_v2:\n",
    "   # Cambia: relevancia >= 0.7\n",
    "   # Prueba: relevancia >= 0.8 (m√°s estricto)\n",
    "   ```\n",
    "\n",
    "3. **Cach√© para respuestas similares:**\n",
    "   - Detecta respuestas casi id√©nticas\n",
    "   - Reutiliza codificaci√≥n previa\n",
    "   - Reduce llamadas a GPT\n",
    "\n",
    "---\n",
    "\n",
    "**¬øPreguntas? Revisa `README_NOTEBOOKS.md` en la carpeta notebooks** üìö\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
